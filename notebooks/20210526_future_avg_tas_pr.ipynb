{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to avg annual tas and pr for Ghana for historical period and for ssp585\n",
    "\n",
    "Author: Sarah Chapman\n",
    "\n",
    "- adapted from Jess Baker's ntoebook (load CMIP6 models)\n",
    "- includes code from Eszter's drought notebook\n",
    "- adapted 'make_cmip6_filepath' function to accomodate multiple model fpaths\n",
    "- code to get the institute for each model\n",
    "\n",
    "- loads pre-processed pr and tas data\n",
    "- calculates avg annual temp and pr for Ghana for mid century time period\n",
    "- also calculates based on rainy season\n",
    "\n",
    "Some useful links\n",
    "https://towardsdatascience.com/basic-data-structures-of-xarray-80bab8094efa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/badc/cmip6/data/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585\n",
      "/badc/cmip6/data/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r1i1p1f2/Amon/pr\n",
      "/badc/cmip6/data/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r1i1p1f2/Amon/pr/gn\n",
      "/badc/cmip6/data/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r1i1p1f2/Amon/pr/gn/v20190507/\n",
      "['pr_Amon_UKESM1-0-LL_ssp585_r1i1p1f2_gn_201501-204912.nc', 'pr_Amon_UKESM1-0-LL_ssp585_r1i1p1f2_gn_205001-210012.nc']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from pyhdf.SD import SD, SDC\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from pathlib import Path\n",
    "import cf_units\n",
    "import pandas as pd\n",
    "import copy\n",
    "import climate_indices\n",
    "from climate_indices import compute, indices\n",
    "import iris\n",
    "from netCDF4 import date2num\n",
    "import iris.coord_categorisation\n",
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "from iris.util import unify_time_units\n",
    "\n",
    "import os \n",
    "\n",
    "def get_dates(cube, verbose=False):\n",
    "    dates = cube.coord('time').units.num2date(cube.coord('time').points)\n",
    "    dates = [dt(date.year, date.month, date.day) for date in dates]\n",
    "    if verbose is True:\n",
    "        print(dates)\n",
    "    else:\n",
    "        print(dates[0], 'â€“', dates[-1])\n",
    "    return(dates)\n",
    "\n",
    "def make_cmip6_filepath(institute, model, scenario, variant, experiment, table_id, variable, grid, version, time_range,\n",
    "                        data_root=\"/badc/cmip6/data/CMIP6/\"):\n",
    "    \"\"\"\n",
    "    Make a file path for a cmip6 dataset for a single variable\n",
    "    Historical runs (1850-2014) are in `/badc/cmip6/data/CMIP6/CMIP/<institute>/<model>/historical/<variant>/<table_id>/<variable>/<grid>/<version>/`\n",
    "    Scenario runs are in `/badc/cmip6/data/CMIP6/ScenarioMIP/<institute>/<model>/<scenario_name>/<variant>/<table_id>/<variable>/<grid>/<version>/`\n",
    "    `scenario_name` is likely to be one of ssp119, ssp126, ssp245, ssp370 or ssp585\n",
    "    `variant` takes the form `r<realiation_id>0<initialization_id>0<physics_id>0<forcing_id>`, e.g. `r1i1p1f2`, where the numbers are the indexes for:  \n",
    "    **r**ealization, **i**nitialization, **p**hysics, **f**orcing\n",
    "    `table_id` generally indicates the frequency of the data, e.g. day, 3hr, Amon\n",
    "    `grid` is the model grid being used, e.g. gn, where  \n",
    "       * `gm`: global mean data  \n",
    "       * `gn`: data reported on a model's native grid  \n",
    "       * `gr1`: regridded data reported on a grid other than the native grid and other than the preferred target grid  \n",
    "    It is likely the `grid` will be the native grid, i.e. `gn`\n",
    "    `version` normally in the form `v[YYYYMMDD]` or `latest`, e.g. `v20200203\n",
    "    \n",
    "    `variable` generally follows the list on https://pcmdi.llnl.gov/mips/cmip3/variableList.html, for example \n",
    "       `tas`: air_temperature \n",
    "       `pr`: precipitation_flux\n",
    "       `ts`: surface_temperature\n",
    "    The following institutions have data in both historical and ScenarioMIPs:\n",
    "    AS-RCEC, AWI, BCC, CAMS, CAS, CCCR-IITM, CCCma, CMCC, CNRM-CERFACS, CSIRO, CSIRO-ARCCSS, E3SM-Project, EC-Earth-Consortium, FIO-QLNM, HAMMOZ-Consortium, INM, IPSL, KIOST, MIROC, MOHC, MPI-M, MRI, NASA-GISS, NCAR, NCC, NIMS-KMA, NOAA-GFDL, NUIST, THU, UA\n",
    "    \"\"\"\n",
    "    # get base path\n",
    "    path = str(DATA_ROOT / scenario / institute / model / experiment)\n",
    "    print(path)\n",
    "    #print(os.listdir(path))\n",
    "    \n",
    "    # get path for variant\n",
    "    if variant is None:\n",
    "        # select first variant\n",
    "        dir_list = os.listdir(path)\n",
    "        variant_list = [x for x in dir_list if x.startswith('r')]\n",
    "    else:\n",
    "        variant_list = [variant]\n",
    "    \n",
    "    # update path\n",
    "    var = [x for x in variant_list if x.startswith('r1i1p1')]\n",
    "    if len(var) == 0:\n",
    "        print(variant_list)\n",
    "        var = [x for x in variant_list if x.startswith('r')]\n",
    "        path = path + '/' + var[0] + '/' + str(table_id) + '/' + str(variable)\n",
    "    else:\n",
    "        path = path + '/' + var[0] + '/' + str(table_id) + '/' + str(variable) \n",
    "    print(path)\n",
    "    # get path for grid\n",
    "    if grid is None:\n",
    "        # select first grid (usually only 1)\n",
    "        dir_list = os.listdir(path)\n",
    "        grid_list = [x for x in dir_list if x.startswith('g')]\n",
    "    else:\n",
    "        grid_list = [grid]\n",
    "        \n",
    "    # update path\n",
    "    path = path + '/' + str(grid_list[0])\n",
    "    print(path)\n",
    "    \n",
    "    # get version path\n",
    "    if version is None:\n",
    "        dir_list2 = os.listdir(path)\n",
    "        version_list = [x for x in dir_list2 if x.startswith('v')]\n",
    "    else:\n",
    "        version_list = [version]\n",
    "    \n",
    "    # update path\n",
    "    path = path + '/' + str(version_list[0]) + '/'\n",
    "    print(path)\n",
    "    print(os.listdir(path))\n",
    "    return(path+ '*.nc')\n",
    "\n",
    "# test\n",
    "DATA_ROOT = Path(\"/badc/cmip6/data/CMIP6/\")\n",
    "model = \"UKESM1-0-LL\"\n",
    "expt = 'ssp585'\n",
    "scenario = 'ScenarioMIP'\n",
    "fp = make_cmip6_filepath(\n",
    "        institute=\"MOHC\", scenario=scenario, model=model, experiment=expt, variant=None,\n",
    "        table_id=\"Amon\", variable=\"pr\", grid=None, version=None, time_range=\"*\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hist_future_cmip6(hist_cube, future_cube):\n",
    "    # combine hist and future data into single cube\n",
    "    hist_dates = get_dates(hist_cube)\n",
    "    calendar = 'gregorian'\n",
    "    units = 'days since ' + str(hist_dates[0].year) + '-01-01 00:00:0.0'\n",
    "    \n",
    "    future_dates = get_dates(future_cube)\n",
    "    \n",
    "    if future_dates[0].year <= hist_dates[-1].year:\n",
    "        # if future and historical cubes overlap remove overlap\n",
    "        print(hist_dates[0].year)\n",
    "        print(future_dates[0].year)\n",
    "        hist_constraint = iris.Constraint(time=lambda cell: hist_dates[0].year <= cell.point.year < future_dates[0].year)\n",
    "        hist_cube = hist_cube.extract(hist_constraint)\n",
    "        hist_dates = get_dates(hist_cube)\n",
    "        \n",
    "    dates = hist_dates + future_dates\n",
    "    \n",
    "    # get time dimension\n",
    "    times = date2num(dates, units=units, calendar=calendar)\n",
    "    time_unit = cf_units.Unit(units, calendar=calendar)\n",
    "    time = iris.coords.DimCoord(times, standard_name='time',\n",
    "                                units=time_unit)\n",
    "\n",
    "    lats = future_cube.coord('latitude').points\n",
    "    latitude = iris.coords.DimCoord(lats, standard_name='latitude',\n",
    "                                    units='degrees')\n",
    "    lons = future_cube.coord('lon').points\n",
    "    longitude = iris.coords.DimCoord(lons, standard_name='longitude',\n",
    "                                     units='degrees')\n",
    "    new_data = np.zeros((len(dates), len(lats), len(lons)))\n",
    "    \n",
    "    # add historical data\n",
    "    new_data[0:hist_cube.shape[0], :, :] = hist_cube.data\n",
    "    new_data[hist_cube.shape[0]:, :, :] = future_cube.data\n",
    "    \n",
    "    # Put data into data cube\n",
    "    cube = iris.cube.Cube(new_data, var_name=future_cube.standard_name,\n",
    "                          units=future_cube.units,\n",
    "                          dim_coords_and_dims=[(time, 0), (latitude, 1),\n",
    "                                               (longitude, 2)])\n",
    "    print(cube)\n",
    "    return(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TaiESM1': 'AS-RCEC', 'UKESM1-0-LL': 'MOHC', 'AWI-CM-1-1-MR': 'AWI', 'AWI-ESM-1-1-LR': 'AWI', 'BCC-CSM2-MR': 'BCC', 'BCC-ESM1': 'BCC', 'CAMS-CSM1-0': 'CAMS', 'CAS-ESM2-0': 'CAS', 'FGOALS-f3-L': 'CAS', 'FGOALS-g3': 'CAS', 'IITM-ESM': 'CCCR-IITM', 'CanESM5': 'CCCma', 'CanESM5-CanOE': 'CCCma', 'CMCC-CM2-HR4': 'CMCC', 'CMCC-CM2-SR5': 'CMCC', 'CMCC-ESM2': 'CMCC', 'CNRM-CM6-1': 'CNRM-CERFACS', 'CNRM-CM6-1-HR': 'CNRM-CERFACS', 'CNRM-ESM2-1': 'CNRM-CERFACS', 'ACCESS-ESM1-5': 'CSIRO', 'ACCESS-CM2': 'CSIRO-ARCCSS', 'E3SM-1-0': 'E3SM-Project', 'E3SM-1-1': 'E3SM-Project', 'E3SM-1-1-ECA': 'E3SM-Project', 'EC-Earth3': 'EC-Earth-Consortium', 'EC-Earth3-AerChem': 'EC-Earth-Consortium', 'EC-Earth3-CC': 'EC-Earth-Consortium', 'EC-Earth3-LR': 'EC-Earth-Consortium', 'EC-Earth3-Veg': 'EC-Earth-Consortium', 'EC-Earth3-Veg-LR': 'EC-Earth-Consortium', 'EC-Earth3P-VHR': 'EC-Earth-Consortium', 'FIO-ESM-2-0': 'FIO-QLNM', 'MPI-ESM-1-2-HAM': 'HAMMOZ-Consortium', 'INM-CM4-8': 'INM', 'INM-CM5-0': 'INM', 'IPSL-CM5A2-INCA': 'IPSL', 'IPSL-CM6A-LR': 'IPSL', 'IPSL-CM6A-LR-INCA': 'IPSL', 'KIOST-ESM': 'KIOST', 'MIROC-ES2H': 'MIROC', 'MIROC-ES2L': 'MIROC', 'MIROC6': 'MIROC', 'HadGEM3-GC31-LL': 'MOHC', 'HadGEM3-GC31-MM': 'MOHC', 'MPI-ESM1-2-HR': 'MPI-M', 'MPI-ESM1-2-LR': 'MPI-M', 'MRI-ESM2-0': 'MRI', 'GISS-E2-1-G': 'NASA-GISS', 'GISS-E2-1-G-CC': 'NASA-GISS', 'GISS-E2-1-H': 'NASA-GISS', 'GISS-E2-2-G': 'NASA-GISS', 'CESM2': 'NCAR', 'CESM2-FV2': 'NCAR', 'CESM2-WACCM': 'NCAR', 'CESM2-WACCM-FV2': 'NCAR', 'NorCPM1': 'NCC', 'NorESM1-F': 'NCC', 'NorESM2-LM': 'NCC', 'NorESM2-MM': 'NCC', 'KACE-1-0-G': 'NIMS-KMA', 'GFDL-AM4': 'NOAA-GFDL', 'GFDL-CM4': 'NOAA-GFDL', 'GFDL-ESM4': 'NOAA-GFDL', 'NESM3': 'NUIST', 'SAM0-UNICON': 'SNU', 'CIESM': 'THU', 'MCM-UA-1-0': 'UA'}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of models and institutes\n",
    "basepath = '/badc/cmip6/data/CMIP6/CMIP/'\n",
    "institute_list = os.listdir(basepath)\n",
    "model_inst_dict = {}\n",
    "\n",
    "# loop over institutes\n",
    "for inst in institute_list:\n",
    "    model_list = os.listdir(basepath + inst + '/')\n",
    "    \n",
    "    # for each institute list models and store in dictionary\n",
    "    for model_temp in model_list:\n",
    "        model_inst_dict[model_temp] = inst\n",
    "        #print(model_inst_dict)\n",
    "        #assert False\n",
    "    \n",
    "    # correction for UKESM which is used by multiple centres - we want MOHC only\n",
    "    model_inst_dict['UKESM1-0-LL'] = 'MOHC'\n",
    "#print(os.listdir(path))\n",
    "print(model_inst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False\n",
    "# useful guide here: https://nci-data-training.readthedocs.io/en/latest/_notebook/climate/1_02_Xarray_subset_slicing_plot_CMIP6.html\n",
    "DATA_ROOT = Path(\"/badc/cmip6/data/CMIP6/\")\n",
    "pr_datasets = {}\n",
    "tas_datasets = {}\n",
    "expt = 'ssp585'\n",
    "scenario = 'ScenarioMIP'\n",
    "\n",
    "# read in monthly data\n",
    "models = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5',\n",
    "          'CNRM-CM6-1', 'CNRM-ESM2-1', 'FGOALS-f3-L', 'FGOALS-g3', 'HadGEM3-GC31-MM',\n",
    "          'GISS-E2-1-G', 'INM-CM5-0', 'INM-CM4-8',\n",
    "          'MPI-ESM1-2-LR', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'] \n",
    "\n",
    "# models without SSP585 pr OR tas or have issues with data ('MRI-ESM2-0', )\n",
    "# 'BCC-ESM1', 'CESM2', 'MPI-ESM-1-2-HAM', 'MPI-ESM1-2-HR', 'NorCPM1', 'MRI-ESM2-0', \n",
    "\n",
    "#for model in [\"HadGEM3-GC31-LL\"]:\n",
    "#for model in ['UKESM1-0-LL']:\n",
    "#for model in ['FGOALS-g3']:\n",
    "for model in models:\n",
    "    print(model)\n",
    "    institute = model_inst_dict[model]\n",
    "    \n",
    "    if model in ['UKESM1-0-LL']:  #something wrong with UKESM r1i1p1 variant (hdf error)\n",
    "        variant = 'r2i1p1f2'\n",
    "    else:\n",
    "        variant = None\n",
    "    \n",
    "    # get historical precip data\n",
    "    fp_hist = make_cmip6_filepath(\n",
    "        institute=institute, scenario='CMIP', model=model, experiment='historical', variant=variant,\n",
    "        table_id=\"Amon\", variable=\"pr\", grid=None, version=None, time_range=\"*\"\n",
    "    )\n",
    "    hist_pr_cube = xr.open_mfdataset(fp_hist)\n",
    "    hist_pr_cube = hist_pr_cube.assign_coords(lon=(((hist_pr_cube.lon + 180) % 360) - 180)).sortby('lon') # change lons from 0,360 to -180,180\n",
    "    \n",
    "    # select data over Ghana and convert to Iris cube\n",
    "    hist_ghana_pr = hist_pr_cube.sel(lat=slice(4.5,11.5), lon=slice(-3.5,1))\n",
    "    hist_ghana_pr = hist_ghana_pr.pr.to_iris()\n",
    "    \n",
    "    fp_future = make_cmip6_filepath(\n",
    "        institute=institute, scenario=scenario, model=model, experiment=expt, variant=variant,\n",
    "        table_id=\"Amon\", variable=\"pr\", grid=None, version=None, time_range=\"*\"\n",
    "    )\n",
    "    \n",
    "    future_pr_cube = xr.open_mfdataset(fp_future)\n",
    "    future_pr_cube = future_pr_cube.assign_coords(lon=(((future_pr_cube.lon + 180) % 360) - 180)).sortby('lon') # change lons from 0,360 to -180,180\n",
    "    \n",
    "    # select data over Ghana and convert to Iris cube\n",
    "    future_ghana_pr = future_pr_cube.sel(lat=slice(4.5,11.5), lon=slice(-3.5,1))\n",
    "    future_ghana_pr = future_ghana_pr.pr.to_iris()\n",
    "    \n",
    "    # combine hist and future precip data into single cube\n",
    "    cube = merge_hist_future_cmip6(hist_ghana_pr, future_ghana_pr)\n",
    "    \n",
    "    pr_datasets[model] = cube\n",
    "    \n",
    "    # repeat for tas\n",
    "    fp_hist = make_cmip6_filepath(\n",
    "        institute=institute, scenario='CMIP', model=model, experiment='historical', variant=variant,\n",
    "        table_id=\"Amon\", variable=\"tas\", grid=None, version=None, time_range=\"*\"\n",
    "    )\n",
    "    hist_tas_cube = xr.open_mfdataset(fp_hist)\n",
    "    \n",
    "    # change longitudes from 0,360 to -180,180\n",
    "    hist_tas_cube = hist_tas_cube.assign_coords(lon=(((hist_tas_cube.lon + 180) % 360) - 180)).sortby('lon')\n",
    "    \n",
    "    # select data over Ghana\n",
    "    hist_ghana_tas = hist_tas_cube.sel(lat=slice(4.5,11.5), lon=slice(-3.5,1))\n",
    "    hist_ghana_tas = hist_ghana_tas.tas.to_iris()\n",
    "    \n",
    "    fp_future = make_cmip6_filepath(\n",
    "        institute=institute, scenario=scenario, model=model, experiment=expt, variant=variant,\n",
    "        table_id=\"Amon\", variable=\"tas\", grid=None, version=None, time_range=\"*\"\n",
    "    )\n",
    "    \n",
    "    future_tas_cube = xr.open_mfdataset(fp_future)\n",
    "    \n",
    "    # change longitudes from 0,360 to -180,180\n",
    "    future_tas_cube = future_tas_cube.assign_coords(lon=(((future_tas_cube.lon + 180) % 360) - 180)).sortby('lon')\n",
    "    \n",
    "    # select data over Ghana\n",
    "    future_ghana_tas = future_tas_cube.sel(lat=slice(4.5,11.5), lon=slice(-3.5,1))\n",
    "    future_ghana_tas = future_ghana_tas.tas.to_iris()\n",
    "    \n",
    "    # combine hist and future data into single cube\n",
    "    cube = merge_hist_future_cmip6(hist_ghana_tas, future_ghana_tas)\n",
    "    \n",
    "    tas_datasets[model] = cube\n",
    "    #assert False\n",
    "outpath = '/home/users/train008/data/'\n",
    "np.save(outpath + 'hist_plus_' + expt + '_ghana_pr_dict.npy', pr_datasets)\n",
    "np.save(outpath + 'hist_plus_' + expt + '_ghana_tas_dict.npy', tas_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_to_frame(cube_dict, expt='hist', region = 'all', var = 'pr'): # set up in case calculate SPEI for multiple models\n",
    "    df = pd.DataFrame(columns = ['model', 'expt', 'year', 'month', 'value', 'region'])\n",
    "    \n",
    "    for key in cube_dict.keys():\n",
    "        cube = cube_dict[key].copy()\n",
    "        \n",
    "        if var == 'pr':\n",
    "            cube.convert_units('kg m-2 month-1')\n",
    "        if var == 'tas':\n",
    "            cube.convert_units('celsius')\n",
    "            \n",
    "        cube = cube.collapsed(['latitude', 'longitude'], iris.analysis.MEAN)\n",
    "            \n",
    "        #cube_time_mean = cube.collapsed(['latitude', 'longitude'], iris.analysis.MEAN)\n",
    "        model = key #extract from attributes later\n",
    "        \n",
    "        #print(cube)\n",
    "        x = cube.data\n",
    "        x = x.flatten() #if masked to land sea, which isn't as moment\n",
    "        \n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "        iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "        \n",
    "        year =  cube.coord('year').points\n",
    "        #print(year)\n",
    "        month = cube.coord('month').points\n",
    "        \n",
    "        y = pd.DataFrame(columns =   ['model', 'expt', 'year', 'month', 'value', 'region'])\n",
    "       \n",
    "        y['value'] = x\n",
    "        y['year'] = year\n",
    "        y['model'] = model\n",
    "        y['expt'] = expt\n",
    "        y['month'] = month\n",
    "        y['region'] = region\n",
    "        \n",
    "        df= df.append(y)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ACCESS-CM2', 'ACCESS-ESM1-5', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CNRM-CM6-1', 'CNRM-ESM2-1', 'FGOALS-f3-L', 'FGOALS-g3', 'HadGEM3-GC31-MM', 'GISS-E2-1-G', 'INM-CM5-0', 'INM-CM4-8', 'MPI-ESM1-2-LR', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'])\n"
     ]
    }
   ],
   "source": [
    "#calculate avg pr and temp in Ghana in future\n",
    "expt = 'ssp585'\n",
    "path = '/home/users/train008/data/'\n",
    "pr_datasets = np.load(path + 'hist_plus_' + expt + '_ghana_pr_dict.npy').item()\n",
    "tas_datasets = np.load(path + 'hist_plus_' + expt + '_ghana_tas_dict.npy').item()\n",
    "print(pr_datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr = cube_to_frame(pr_datasets)\n",
    "df_tas = cube_to_frame(tas_datasets, var = 'tas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average tas and monthly rainfall in historical\n",
    "\n",
    "df_pr_his = df_pr[(df_pr['year'] >= 1980) & (df_pr['year'] <= 2010)]\n",
    "df_tas_his = df_tas[(df_tas['year'] >= 1980) & (df_tas['year'] <= 2010)]\n",
    "\n",
    "df_pr_mid = df_pr[(df_pr['year'] >= 2040) & (df_pr['year'] <= 2050)]\n",
    "df_tas_mid = df_tas[(df_tas['year'] >= 2040) & (df_tas['year'] <= 2050)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annual precipitation (mm) in 2040 - 2050  1064.0\n",
      "Avg annual temperature (C)  27.0\n",
      "Total annual precipitation (mm) in 2040 - 2050  1112.0\n",
      "Avg annual temperature (C)  29.0\n"
     ]
    }
   ],
   "source": [
    "x = df_pr_his.groupby(['month'])['value'].mean()\n",
    "x\n",
    "np.nansum(x.values)\n",
    "\n",
    "print('Total annual precipitation (mm) in 2040 - 2050 ', np.round(np.nansum(x.values),0))\n",
    "print('Avg annual temperature (C) ', np.round(np.nanmean(df_tas_his['value']),0))\n",
    "\n",
    "x = df_pr_mid.groupby(['month'])['value'].mean()\n",
    "x\n",
    "\n",
    "\n",
    "print('Total annual precipitation (mm) in 2040 - 2050 ', np.round(np.nansum(x.values),0))\n",
    "print('Avg annual temperature (C) ', np.round(np.nanmean(df_tas_mid['value']),0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at wet season only\n",
    "wet_season = ['Jun', 'Jul', 'Aug', 'Sep']\n",
    "\n",
    "df_pr_his = df_pr_his[df_pr_his['month'].isin(wet_season)]\n",
    "df_tas_his = df_tas_his[df_tas_his['month'].isin(wet_season)]\n",
    "\n",
    "df_pr_mid = df_pr_mid[df_pr_mid['month'].isin(wet_season)]\n",
    "df_tas_mid = df_tas_mid[df_tas_mid['month'].isin(wet_season)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_his.groupby(['month'])['value'].mean()\n",
    "df_pr_his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170.80215186324713\n",
      "177.0018462685923\n",
      "26.520434846149612\n",
      "28.077663215746863\n"
     ]
    }
   ],
   "source": [
    "print(np.nanmean(df_pr_his['value']))\n",
    "print(np.nanmean(df_pr_mid['value']))\n",
    "\n",
    "print(np.nanmean(df_tas_his['value']))\n",
    "print(np.nanmean(df_tas_mid['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.94483951639248\n",
      "30.235563853857357\n"
     ]
    }
   ],
   "source": [
    "tas_his_max = df_tas_his.groupby(['model', 'expt'])['value'].max()\n",
    "tas_mid_max = df_tas_mid.groupby(['model', 'expt'])['value'].max()\n",
    "\n",
    "print(np.nanmean(tas_his_max.values))\n",
    "print(np.nanmean(tas_mid_max.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aug', 'Jul', 'Jun', 'Sep'], dtype=object)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_pr_his['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
