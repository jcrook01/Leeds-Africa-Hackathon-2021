{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to calculate historical SPEI (multiple models)\n",
    "\n",
    "- adapted from Jess Baker's code\n",
    "- adapted from Sarah's Chapman's notebook\n",
    "- includes code from Eszter's drought notebook\n",
    "- adapted 'make_cmip6_filepath' function to accomodate multiple model fpaths\n",
    "- code to get the institute for each model\n",
    "- calculates average and max intensity and duration in models, and for the ensemble mean\n",
    "\n",
    "Some useful links\n",
    "https://towardsdatascience.com/basic-data-structures-of-xarray-80bab8094efa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/badc/cmip6/data/CMIP6/CMIP/MOHC/UKESM1-0-LL/historical/r1i1p1f2/Amon/pr/gn/v20190406/\n",
      "['pr_Amon_UKESM1-0-LL_historical_r1i1p1f2_gn_185001-194912.nc', 'pr_Amon_UKESM1-0-LL_historical_r1i1p1f2_gn_195001-201412.nc']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from pyhdf.SD import SD, SDC\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import copy\n",
    "import climate_indices\n",
    "from climate_indices import compute, indices\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import os \n",
    "\n",
    "def get_dates(cube, verbose=False):\n",
    "    dates = cube.coord('time').units.num2date(cube.coord('time').points)\n",
    "    dates = [dt(date.year, date.month, date.day) for date in dates]\n",
    "    if verbose is True:\n",
    "        print(dates)\n",
    "    else:\n",
    "        print(dates[0], '–', dates[-1])\n",
    "    return(dates)\n",
    "\n",
    "def make_cmip6_filepath(institute, model, scenario, variant, experiment, table_id, variable, grid, version, time_range,\n",
    "                        data_root=\"/badc/cmip6/data/CMIP6/\"):\n",
    "    \"\"\"\n",
    "    Make a file path for a cmip6 dataset for a single variable\n",
    "    Historical runs (1850-2014) are in `/badc/cmip6/data/CMIP6/CMIP/<institute>/<model>/historical/<variant>/<table_id>/<variable>/<grid>/<version>/`\n",
    "    Scenario runs are in `/badc/cmip6/data/CMIP6/ScenarioMIP/<institute>/<model>/<scenario_name>/<variant>/<table_id>/<variable>/<grid>/<version>/`\n",
    "    `scenario_name` is likely to be one of ssp119, ssp126, ssp245, ssp370 or ssp585\n",
    "    `variant` takes the form `r<realiation_id>0<initialization_id>0<physics_id>0<forcing_id>`, e.g. `r1i1p1f2`, where the numbers are the indexes for:  \n",
    "    **r**ealization, **i**nitialization, **p**hysics, **f**orcing\n",
    "    `table_id` generally indicates the frequency of the data, e.g. day, 3hr, Amon\n",
    "    `grid` is the model grid being used, e.g. gn, where  \n",
    "       * `gm`: global mean data  \n",
    "       * `gn`: data reported on a model's native grid  \n",
    "       * `gr1`: regridded data reported on a grid other than the native grid and other than the preferred target grid  \n",
    "    It is likely the `grid` will be the native grid, i.e. `gn`\n",
    "    `version` normally in the form `v[YYYYMMDD]` or `latest`, e.g. `v20200203\n",
    "    \n",
    "    `variable` generally follows the list on https://pcmdi.llnl.gov/mips/cmip3/variableList.html, for example \n",
    "       `tas`: air_temperature \n",
    "       `pr`: precipitation_flux\n",
    "       `ts`: surface_temperature\n",
    "    The following institutions have data in both historical and ScenarioMIPs:\n",
    "    AS-RCEC, AWI, BCC, CAMS, CAS, CCCR-IITM, CCCma, CMCC, CNRM-CERFACS, CSIRO, CSIRO-ARCCSS, E3SM-Project, EC-Earth-Consortium, FIO-QLNM, HAMMOZ-Consortium, INM, IPSL, KIOST, MIROC, MOHC, MPI-M, MRI, NASA-GISS, NCAR, NCC, NIMS-KMA, NOAA-GFDL, NUIST, THU, UA\n",
    "    \"\"\"\n",
    "    # get base path\n",
    "    path = str(DATA_ROOT / scenario / institute / model / experiment)\n",
    "    #print(os.listdir(path))\n",
    "    \n",
    "    # get path for variant\n",
    "    if variant is None:\n",
    "        # select first variant\n",
    "        dir_list = os.listdir(path)\n",
    "        variant_list = [x for x in dir_list if x.startswith('r')]\n",
    "    else:\n",
    "        variant_list = [variant]\n",
    "    \n",
    "    # update path\n",
    "    var = [x for x in variant_list if x.startswith('r1i1p1')]\n",
    "    if len(var) == 0:\n",
    "        print(variant_list)\n",
    "        var = [x for x in variant_list if x.startswith('r')]\n",
    "        path = path + '/' + var[0] + '/' + str(table_id) + '/' + str(variable)\n",
    "    else:\n",
    "        path = path + '/' + var[0] + '/' + str(table_id) + '/' + str(variable) \n",
    "        #print(path)\n",
    "    # get path for grid\n",
    "    if grid is None:\n",
    "        # select first grid (usually only 1)\n",
    "        dir_list = os.listdir(path)\n",
    "        grid_list = [x for x in dir_list if x.startswith('g')]\n",
    "    else:\n",
    "        grid_list = [grid]\n",
    "        \n",
    "    # update path\n",
    "    path = path + '/' + str(grid_list[0])\n",
    "    \n",
    "    # get version path\n",
    "    if version is None:\n",
    "        dir_list2 = os.listdir(path)\n",
    "        version_list = [x for x in dir_list2 if x.startswith('v')]\n",
    "    else:\n",
    "        version_list = [version]\n",
    "    \n",
    "    # update path\n",
    "    path = path + '/' + str(version_list[0]) + '/'\n",
    "    print(path)\n",
    "    print(os.listdir(path))\n",
    "    return(path+ '*.nc')\n",
    "\n",
    "# test\n",
    "DATA_ROOT = Path(\"/badc/cmip6/data/CMIP6/\")\n",
    "model = \"UKESM1-0-LL\"\n",
    "fp = make_cmip6_filepath(\n",
    "        institute=\"MOHC\", scenario='CMIP', model=model, experiment='historical', variant=None,\n",
    "        table_id=\"Amon\", variable=\"pr\", grid=None, version=None, time_range=\"*\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TaiESM1': 'AS-RCEC', 'UKESM1-0-LL': 'MOHC', 'AWI-CM-1-1-MR': 'AWI', 'AWI-ESM-1-1-LR': 'AWI', 'BCC-CSM2-MR': 'BCC', 'BCC-ESM1': 'BCC', 'CAMS-CSM1-0': 'CAMS', 'CAS-ESM2-0': 'CAS', 'FGOALS-f3-L': 'CAS', 'FGOALS-g3': 'CAS', 'IITM-ESM': 'CCCR-IITM', 'CanESM5': 'CCCma', 'CanESM5-CanOE': 'CCCma', 'CMCC-CM2-HR4': 'CMCC', 'CMCC-CM2-SR5': 'CMCC', 'CMCC-ESM2': 'CMCC', 'CNRM-CM6-1': 'CNRM-CERFACS', 'CNRM-CM6-1-HR': 'CNRM-CERFACS', 'CNRM-ESM2-1': 'CNRM-CERFACS', 'ACCESS-ESM1-5': 'CSIRO', 'ACCESS-CM2': 'CSIRO-ARCCSS', 'E3SM-1-0': 'E3SM-Project', 'E3SM-1-1': 'E3SM-Project', 'E3SM-1-1-ECA': 'E3SM-Project', 'EC-Earth3': 'EC-Earth-Consortium', 'EC-Earth3-AerChem': 'EC-Earth-Consortium', 'EC-Earth3-CC': 'EC-Earth-Consortium', 'EC-Earth3-LR': 'EC-Earth-Consortium', 'EC-Earth3-Veg': 'EC-Earth-Consortium', 'EC-Earth3-Veg-LR': 'EC-Earth-Consortium', 'EC-Earth3P-VHR': 'EC-Earth-Consortium', 'FIO-ESM-2-0': 'FIO-QLNM', 'MPI-ESM-1-2-HAM': 'HAMMOZ-Consortium', 'INM-CM4-8': 'INM', 'INM-CM5-0': 'INM', 'IPSL-CM5A2-INCA': 'IPSL', 'IPSL-CM6A-LR': 'IPSL', 'IPSL-CM6A-LR-INCA': 'IPSL', 'KIOST-ESM': 'KIOST', 'MIROC-ES2H': 'MIROC', 'MIROC-ES2L': 'MIROC', 'MIROC6': 'MIROC', 'HadGEM3-GC31-LL': 'MOHC', 'HadGEM3-GC31-MM': 'MOHC', 'MPI-ESM1-2-HR': 'MPI-M', 'MPI-ESM1-2-LR': 'MPI-M', 'MRI-ESM2-0': 'MRI', 'GISS-E2-1-G': 'NASA-GISS', 'GISS-E2-1-G-CC': 'NASA-GISS', 'GISS-E2-1-H': 'NASA-GISS', 'GISS-E2-2-G': 'NASA-GISS', 'CESM2': 'NCAR', 'CESM2-FV2': 'NCAR', 'CESM2-WACCM': 'NCAR', 'CESM2-WACCM-FV2': 'NCAR', 'NorCPM1': 'NCC', 'NorESM1-F': 'NCC', 'NorESM2-LM': 'NCC', 'NorESM2-MM': 'NCC', 'KACE-1-0-G': 'NIMS-KMA', 'GFDL-AM4': 'NOAA-GFDL', 'GFDL-CM4': 'NOAA-GFDL', 'GFDL-ESM4': 'NOAA-GFDL', 'NESM3': 'NUIST', 'SAM0-UNICON': 'SNU', 'CIESM': 'THU', 'MCM-UA-1-0': 'UA'}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of models and institutes\n",
    "basepath = '/badc/cmip6/data/CMIP6/CMIP/'\n",
    "institute_list = os.listdir(basepath)\n",
    "model_inst_dict = {}\n",
    "\n",
    "# loop over institutes\n",
    "for inst in institute_list:\n",
    "    model_list = os.listdir(basepath + inst + '/')\n",
    "    \n",
    "    # for each institute list models and store in dictionary\n",
    "    for model_temp in model_list:\n",
    "        model_inst_dict[model_temp] = inst\n",
    "        #print(model_inst_dict)\n",
    "        #assert False\n",
    "    \n",
    "    # correction for UKESM which is used by multiple centres - we want MOHC only\n",
    "    model_inst_dict['UKESM1-0-LL'] = 'MOHC'\n",
    "#print(os.listdir(path))\n",
    "print(model_inst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a file -- NEED TO CHANGE REGION\n",
    "assert False\n",
    "\n",
    "DATA_ROOT = Path(\"/badc/cmip6/data/CMIP6/\")\n",
    "pr_datasets = {}\n",
    "tas_datasets = {}\n",
    "expt = \"historical\" #\"ssp585\"\n",
    "scenario = 'CMIP' # \"ScenarioMIP\"\n",
    "# read in monthly data\n",
    "models = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'BCC-CSM2-MR', 'BCC-ESM1', 'CAMS-CSM1-0', 'CanESM5',\n",
    "          'CESM2', 'CNRM-CM6-1', 'CNRM-ESM2-1', 'FGOALS-f3-L', 'FGOALS-g3', 'HadGEM3-GC31-MM',\n",
    "          'GISS-E2-1-G', 'INM-CM5-0', 'INM-CM4-8', 'MPI-ESM-1-2-HAM',\n",
    "          'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NorCPM1', 'NorESM2-LM',\n",
    "          'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'] \n",
    "\n",
    "#for model in [\"HadGEM3-GC31-LL\"]:\n",
    "#for model in ['UKESM1-0-LL']:\n",
    "for model in models:\n",
    "    print(model)\n",
    "    institute = model_inst_dict[model]\n",
    "    print(institute)\n",
    "    \n",
    "    if model in ['UKESM1-0-LL']:  #something wrong with UKESM r1i1p1 variant (hdf error)\n",
    "        variant = 'r2i1p1f2'\n",
    "    else:\n",
    "        variant = None\n",
    "    fp = make_cmip6_filepath(\n",
    "        institute=institute, scenario=scenario, model=model, experiment=expt, variant=variant,\n",
    "        table_id=\"Amon\", variable=\"pr\", grid=None, version=None, time_range=\"*\"\n",
    "    )\n",
    "    \n",
    "    pr_cube = xr.open_mfdataset(fp)\n",
    "    \n",
    "    # useful guide here: https://nci-data-training.readthedocs.io/en/latest/_notebook/climate/1_02_Xarray_subset_slicing_plot_CMIP6.html\n",
    "    # change longitudes from 0,360 to -180,180\n",
    "    pr_cube = pr_cube.assign_coords(lon=(((pr_cube.lon + 180) % 360) - 180)).sortby('lon')\n",
    "    \n",
    "    # select data over Ghana\n",
    "    ghana_pr = pr_cube.sel(lat=slice(4.5,11.5), lon=slice(-3.5,1))\n",
    "    #print(ghana_pr.pr)\n",
    "    \n",
    "    pr_datasets[model] = ghana_pr\n",
    "    \n",
    "    fp = make_cmip6_filepath(\n",
    "        institute=institute, scenario=scenario, model=model, experiment=expt, variant=variant,\n",
    "        table_id=\"Amon\", variable=\"tas\", grid=None, version=None, time_range=\"*\"\n",
    "    )\n",
    "    print(fp)\n",
    "    tas_cube = xr.open_mfdataset(fp)\n",
    "    \n",
    "    tas_cube = tas_cube.assign_coords(lon=(((tas_cube.lon + 180) % 360) - 180)).sortby('lon')\n",
    "    \n",
    "    # select data over Ghana\n",
    "    ghana_tas = tas_cube.sel(lat=slice(4.5,11.4), lon=slice(-3.5,1))\n",
    "    #print(ghana_tas.tas)\n",
    "    tas_datasets[model] = ghana_tas\n",
    "    #assert False\n",
    "outpath = '/home/users/train008/data/'\n",
    "np.save(outpath+'historical_ghana_pr_dict.npy', pr_datasets)\n",
    "np.save(outpath+'historical_ghana_tas_dict.npy', tas_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ACCESS-CM2', 'ACCESS-ESM1-5', 'BCC-CSM2-MR', 'BCC-ESM1', 'CAMS-CSM1-0', 'CanESM5', 'CESM2', 'CNRM-CM6-1', 'CNRM-ESM2-1', 'FGOALS-f3-L', 'FGOALS-g3', 'HadGEM3-GC31-MM', 'GISS-E2-1-G', 'INM-CM5-0', 'INM-CM4-8', 'MPI-ESM-1-2-HAM', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NorCPM1', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'])\n"
     ]
    }
   ],
   "source": [
    "# load dictionaries saved in previous cell\n",
    "path = '/home/users/train008/data/'\n",
    "pr_datasets = np.load(path+'historical_ghana_pr_dict.npy').item()\n",
    "tas_datasets = np.load(path+'historical_ghana_tas_dict.npy').item()\n",
    "print(pr_datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation_flux / (kg m-2 s-1)   (time: 360; latitude: 13; lon: 5)\n",
      "     Dimension coordinates:\n",
      "          time                           x              -        -\n",
      "          latitude                       -              x        -\n",
      "          lon                            -              -        x\n",
      "     Attributes:\n",
      "          cell_measures: area: areacella\n",
      "          comment: includes both liquid and solid phases\n",
      "          original_name: mo: (stash: m01s05i216, lbproc: 128)\n",
      "     Cell methods:\n",
      "          mean: area, time\n",
      "1981-01-16 00:00:00 – 2010-12-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#set up variables\n",
    "# convert to iris cube and extract lat and lon arrays\n",
    "time_constraint = iris.Constraint(time=lambda cell: 1981 <= cell.point.year <= 2010)\n",
    "constraint_lon = iris.Constraint(longitude = lambda cell: 6 <= cell.point < 8)\n",
    "constraint_lat = iris.Constraint(latitude = lambda cell: -3 <= cell.point < 1)\n",
    "constraint = time_constraint  & constraint_lon & constraint_lat \n",
    "\n",
    "# convert to iris cube and extract lat and lon arrays\n",
    "\n",
    "pr_temp = pr_datasets[\"HadGEM3-GC31-MM\"].pr.to_iris().extract(time_constraint)\n",
    "print(pr_temp)\n",
    "\n",
    "lon = pr_temp.coord('lon').points\n",
    "lat = pr_temp.coord('latitude').points\n",
    "\n",
    "dates = get_dates(pr_temp)\n",
    "startyear = dates[0].year\n",
    "calib_year_init = 1980\n",
    "calib_year_final = 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2\n",
      "(360, 5, 3)\n",
      "(360, 5, 3)\n",
      "[ 5.625  6.875  8.125  9.375 10.625]\n",
      "(360, 5, 3)\n",
      "[ 5.625  6.875  8.125  9.375 10.625]\n",
      "ACCESS-ESM1-5\n",
      "(360, 6, 2)\n",
      "(360, 6, 2)\n",
      "[ 5.    6.25  7.5   8.75 10.   11.25]\n",
      "(360, 6, 2)\n",
      "[ 5.    6.25  7.5   8.75 10.   11.25]\n",
      "BCC-CSM2-MR\n",
      "(360, 6, 4)\n",
      "(360, 6, 4)\n",
      "[ 5.04670442  6.16819425  7.28968406  8.41117384  9.53266359 10.65415331]\n",
      "(360, 6, 4)\n",
      "[ 5.04670442  6.16819425  7.28968406  8.41117384  9.53266359 10.65415331]\n",
      "BCC-ESM1\n",
      "(360, 2, 2)\n",
      "(360, 2, 2)\n",
      "[6.97653355 9.76714556]\n",
      "(360, 2, 2)\n",
      "[6.97653355 9.76714556]\n",
      "CAMS-CSM1-0\n",
      "(360, 6, 4)\n",
      "(360, 6, 4)\n",
      "[ 5.04670442  6.16819425  7.28968406  8.41117384  9.53266359 10.65415331]\n",
      "(360, 6, 4)\n",
      "[ 5.04670442  6.16819425  7.28968406  8.41117384  9.53266359 10.65415331]\n",
      "CanESM5\n",
      "(360, 2, 2)\n",
      "(360, 2, 2)\n",
      "[6.97653355 9.76714556]\n",
      "(360, 2, 2)\n",
      "[6.97653355 9.76714556]\n",
      "CESM2\n",
      "(360, 7, 3)\n",
      "(360, 7, 3)\n",
      "[ 5.18324607  6.12565445  7.06806283  8.0104712   8.95287958  9.89528796\n",
      " 10.83769634]\n",
      "(360, 7, 3)\n",
      "[ 5.18324607  6.12565445  7.06806283  8.0104712   8.95287958  9.89528796\n",
      " 10.83769634]\n",
      "CNRM-CM6-1\n",
      "(360, 5, 3)\n",
      "(360, 5, 3)\n",
      "[ 4.90268653  6.30345404  7.70422148  9.10498886 10.50575615]\n",
      "(360, 5, 3)\n",
      "[ 4.90268653  6.30345404  7.70422148  9.10498886 10.50575615]\n",
      "CNRM-ESM2-1\n",
      "(360, 5, 3)\n",
      "(360, 5, 3)\n",
      "[ 4.90268653  6.30345404  7.70422148  9.10498886 10.50575615]\n",
      "(360, 5, 3)\n",
      "[ 4.90268653  6.30345404  7.70422148  9.10498886 10.50575615]\n",
      "FGOALS-f3-L\n",
      "(360, 8, 4)\n",
      "(360, 8, 4)\n",
      "[ 4.5  5.5  6.5  7.5  8.5  9.5 10.5 11.5]\n",
      "(360, 7, 4)\n",
      "[ 4.5  5.5  6.5  7.5  8.5  9.5 10.5]\n",
      "FGOALS-f3-L ERROR WITH INDEXES - check shape of tas_ds and x_pr\n",
      "pr_ds shape (360, 8, 4)\n",
      "tas_ds shape (360, 7, 4)\n",
      "FGOALS-g3\n",
      "(360, 4, 2)\n",
      "(360, 4, 2)\n",
      "[ 5.06329114  7.08860759  9.11392405 11.13924051]\n",
      "(360, 4, 2)\n",
      "[ 5.06329114  7.08860759  9.11392405 11.13924051]\n",
      "HadGEM3-GC31-MM\n",
      "(360, 13, 5)\n",
      "(360, 13, 5)\n",
      "[ 4.722229    5.27778625  5.83333588  6.38889313  6.94445038  7.50000763\n",
      "  8.05556488  8.6111145   9.16667175  9.722229   10.27778625 10.83333588\n",
      " 11.38889313]\n",
      "(360, 13, 5)\n",
      "[ 4.722229    5.27778625  5.83333588  6.38889313  6.94445038  7.50000763\n",
      "  8.05556488  8.6111145   9.16667175  9.722229   10.27778625 10.83333588\n",
      " 11.38889313]\n",
      "GISS-E2-1-G\n",
      "(360, 4, 1)\n",
      "(360, 4, 1)\n",
      "[ 5.  7.  9. 11.]\n",
      "(360, 4, 1)\n",
      "[ 5.  7.  9. 11.]\n",
      "INM-CM5-0\n",
      "(360, 5, 2)\n",
      "(360, 5, 2)\n",
      "[ 5.25  6.75  8.25  9.75 11.25]\n",
      "(360, 5, 2)\n",
      "[ 5.25  6.75  8.25  9.75 11.25]\n",
      "INM-CM4-8\n",
      "(360, 5, 2)\n",
      "(360, 5, 2)\n",
      "[ 5.25  6.75  8.25  9.75 11.25]\n",
      "(360, 5, 2)\n",
      "[ 5.25  6.75  8.25  9.75 11.25]\n",
      "MPI-ESM-1-2-HAM\n",
      "(360, 4, 2)\n",
      "(360, 4, 2)\n",
      "[ 4.66314971  6.5284094   8.39366891 10.25892817]\n",
      "(360, 4, 2)\n",
      "[ 4.66314971  6.5284094   8.39366891 10.25892817]\n",
      "MPI-ESM1-2-HR\n",
      "(360, 7, 5)\n",
      "(360, 7, 5)\n",
      "[ 5.14283975  6.0779015   7.01296324  7.94802496  8.88308668  9.81814837\n",
      " 10.75321005]\n",
      "(360, 7, 5)\n",
      "[ 5.14283975  6.0779015   7.01296324  7.94802496  8.88308668  9.81814837\n",
      " 10.75321005]\n",
      "MPI-ESM1-2-LR\n",
      "(360, 4, 2)\n",
      "(360, 4, 2)\n",
      "[ 4.66314971  6.5284094   8.39366891 10.25892817]\n",
      "(360, 4, 2)\n",
      "[ 4.66314971  6.5284094   8.39366891 10.25892817]\n",
      "MRI-ESM2-0\n",
      "(360, 6, 4)\n",
      "(360, 6, 4)\n",
      "[ 5.0467   6.16819  7.28968  8.41117  9.53266 10.65415]\n",
      "(360, 6, 4)\n",
      "[ 5.0467   6.16819  7.28968  8.41117  9.53266 10.65415]\n",
      "NorCPM1\n",
      "(360, 8, 2)\n",
      "(360, 8, 2)\n",
      "[ 4.73684211  4.73684211  6.63157895  6.63157895  8.52631579  8.52631579\n",
      " 10.42105263 10.42105263]\n",
      "(360, 8, 2)\n",
      "[ 4.73684211  4.73684211  6.63157895  6.63157895  8.52631579  8.52631579\n",
      " 10.42105263 10.42105263]\n",
      "NorESM2-LM\n",
      "(360, 4, 2)\n",
      "(360, 4, 2)\n",
      "[ 4.73684211  6.63157895  8.52631579 10.42105263]\n",
      "(360, 4, 2)\n",
      "[ 4.73684211  6.63157895  8.52631579 10.42105263]\n",
      "NorESM2-MM\n",
      "(360, 7, 3)\n",
      "(360, 7, 3)\n",
      "[ 5.18324607  6.12565445  7.06806283  8.0104712   8.95287958  9.89528796\n",
      " 10.83769634]\n",
      "(360, 7, 3)\n",
      "[ 5.18324607  6.12565445  7.06806283  8.0104712   8.95287958  9.89528796\n",
      " 10.83769634]\n",
      "TaiESM1\n",
      "(360, 7, 3)\n",
      "(360, 7, 3)\n",
      "[ 5.18324607  6.12565445  7.06806283  8.0104712   8.95287958  9.89528796\n",
      " 10.83769634]\n",
      "(360, 7, 3)\n",
      "[ 5.18324607  6.12565445  7.06806283  8.0104712   8.95287958  9.89528796\n",
      " 10.83769634]\n",
      "UKESM1-0-LL\n",
      "(360, 5, 3)\n",
      "(360, 5, 3)\n",
      "[ 5.625  6.875  8.125  9.375 10.625]\n",
      "(360, 5, 3)\n",
      "[ 5.625  6.875  8.125  9.375 10.625]\n"
     ]
    }
   ],
   "source": [
    "#assert False\n",
    "spei_dict = {}\n",
    "spei_north_dict = {}\n",
    "spei_south_dict = {}\n",
    "hist_constraint = iris.Constraint(time=lambda cell: 1981 <= cell.point.year <= 2010)\n",
    "\n",
    "for key in pr_datasets.keys():\n",
    "    print(key)\n",
    "    \n",
    "    pr_temp = pr_datasets[key].pr.to_iris().extract(hist_constraint)\n",
    "    tas_temp = tas_datasets[key].tas.to_iris().extract(hist_constraint)\n",
    "    dims = pr_temp.shape\n",
    "    print(dims)\n",
    "    \n",
    "    #create cubes for storing data\n",
    "    spei_cube = copy.deepcopy(pr_temp)  #np.array((pr_temp.shape))\n",
    "    pet_cube = copy.deepcopy(pr_temp)   #np.array((pr_temp.shape))\n",
    "\n",
    "    pr_ds = pr_datasets[key].pr.to_iris()\n",
    "    pr_ds = pr_ds.extract(hist_constraint)\n",
    "    print(pr_ds.shape)\n",
    "    pr_ds.convert_units('kg m-2 day-1')\n",
    "    print(pr_ds.coord('latitude').points)\n",
    "    \n",
    "    tas_ds = tas_datasets[key].tas.to_iris()\n",
    "    tas_ds = tas_ds.extract(hist_constraint)\n",
    "    print(tas_ds.shape)\n",
    "    tas_ds.convert_units('Celsius')\n",
    "    print(tas_ds.coord('latitude').points)\n",
    "    \n",
    "    try:\n",
    "        for j in np.arange(dims[1]):\n",
    "            for k in np.arange(dims[2]):\n",
    "            \n",
    "                x_pr = pr_ds[:, j, k].data\n",
    "                x_tas = tas_ds[:, j, k].data\n",
    "                #assert False\n",
    "\n",
    "                temp_lat = pr_ds[0, j, k].coord('latitude').points[0]\n",
    "\n",
    "                #calculate pet\n",
    "                pet = indices.pet(x_tas, temp_lat, startyear)\n",
    "                pet_cube.data[:,j,k] = pet\n",
    "\n",
    "                #calcualte spei\n",
    "                kwargs = dict(\n",
    "                    precips_mm = x_pr,\n",
    "                    pet_mm = pet[0:dims[0]],\n",
    "                    scale = 1, # months over which to calculate SPEI, i.e., 3 month running mean etc\n",
    "                    distribution = indices.Distribution.gamma,\n",
    "                    periodicity = indices.compute.Periodicity.monthly, #time scale of input data (needs to be monthly for SPEI)\n",
    "                    data_start_year = startyear,\n",
    "                    calibration_year_initial = calib_year_init,\n",
    "                    calibration_year_final = calib_year_final,\n",
    "                    fitting_params=None\n",
    "                )\n",
    "            spei = indices.spei(**kwargs)\n",
    "\n",
    "            spei_cube.data[:,j,k] = spei\n",
    "            #assert False\n",
    "        \n",
    "        \n",
    "        \n",
    "        #save north and south\n",
    "        spei_north = spei_cube.extract(iris.Constraint(latitude = lambda cell: cell >= 8.0))\n",
    "        spei_south = spei_cube.extract(iris.Constraint(latitude = lambda cell: cell <= 8.0))\n",
    "        \n",
    "        #try:\n",
    "        #    spei_nmean = spei_north.collapsed(['latitude', 'longitude'], iris.analysis.MEAN)\n",
    "        #    spei_smean = spei_south.collapsed(['latitude', 'longitude'], iris.analysis.MEAN)\n",
    "        #    spei_mean = spei_cube.collapsed(['latitude', 'longitude'], iris.analysis.MEAN)\n",
    "        #except:\n",
    "        #    print('no longitude, latitude')\n",
    "            \n",
    "        try:\n",
    "            spei_nmean = spei_north.collapsed(['latitude', 'lon'], iris.analysis.MEAN)\n",
    "            spei_smean = spei_south.collapsed(['latitude', 'lon'], iris.analysis.MEAN)\n",
    "            spei_mean = spei_cube.collapsed(['latitude', 'lon'], iris.analysis.MEAN)\n",
    "        except:\n",
    "            print('no lon, latitude')\n",
    "        \n",
    "        spei_dict[key] = spei_mean\n",
    "       \n",
    "        spei_north_dict[key] = spei_nmean\n",
    "        spei_south_dict[key] = spei_smean\n",
    "    \n",
    "    except IndexError:\n",
    "        print(key, 'ERROR WITH INDEXES - check shape of tas_ds and x_pr')\n",
    "        print('pr_ds shape', pr_ds.shape)\n",
    "        print('tas_ds shape', tas_ds.shape)\n",
    "        continue\n",
    "    \n",
    "outpath = '/home/users/train008/data/'\n",
    "np.save(outpath+'historical_ghana_meanspei_dict_calib1980_2010.npy', spei_dict)\n",
    "np.save(outpath+'historical_ghana_meanspei_south_dict_calib1980_2010.npy', spei_north_dict)\n",
    "np.save(outpath+'historical_ghana_meanspei_north_dict_calib1980_2010.npy', spei_south_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dicts save in previous cell\n",
    "path = '/home/users/train008/data/'\n",
    "spei_dict = np.load(path+'historical_ghana_meanspei_dict_calib1980_2010.npy').item()\n",
    "spei_ndict = np.load(path+'historical_ghana_meanspei_north_dict_calib1980_2010.npy').item()\n",
    "spei_sdict = np.load(path+'historical_ghana_meanspei_south_dict_calib1980_2010.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_to_frame(cube_dict, expt='hist', region = 'all'): # set up in case calculate SPEI for multiple models\n",
    "    df = pd.DataFrame(columns = ['model', 'expt', 'year', 'month', 'value', 'region'])\n",
    "    \n",
    "    for key in cube_dict.keys():\n",
    "        cube = cube_dict[key].copy()\n",
    "        model = key #extract from attributes later\n",
    "        \n",
    "        x = cube.data\n",
    "        x = x[~x.mask].flatten() #if masked to land sea, which isn't as moment\n",
    "        \n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "        iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "        \n",
    "        year =  cube.coord('year').points\n",
    "        #print(year)\n",
    "        month = cube.coord('month').points\n",
    "        \n",
    "        y = pd.DataFrame(columns =   ['model', 'expt', 'year', 'month', 'value', 'region'])\n",
    "       \n",
    "        y['value'] = x\n",
    "        y['year'] = year\n",
    "        y['model'] = model\n",
    "        y['expt'] = expt\n",
    "        y['month'] = month\n",
    "        y['region'] = region\n",
    "        \n",
    "        df= df.append(y)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for all regions\n",
    "df = pd.DataFrame(columns = ['model', 'expt', 'year', 'month', 'value', 'region'])\n",
    "dict_list = [spei_dict, spei_ndict, spei_sdict]\n",
    "reg_list = ['all', 'north', 'south']\n",
    "for i in np.arange(len(dict_list)):\n",
    "    d = dict_list[i]\n",
    "    r = reg_list[i]\n",
    "    y = cube_to_frame(d, region = r)\n",
    "    df = df.append(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate drought characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drought intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##number of months SPEI <= -3\n",
    "df_1 = df[df['value'] <= -1]\n",
    "df_3 = df[df['value'] <= -3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models in the present day only have 'average' droughts. The average intensity of droughts across the ensemble mean is -1.3.\n",
    "No models have an extreme drought (SPEI < -3) in the historical period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.299513083753428\n"
     ]
    }
   ],
   "source": [
    "df_1.groupby(['model', 'expt'])['value'].min()\n",
    "\n",
    "print(np.nanmean(df_1['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: value, dtype: int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.groupby(['model', 'expt', 'region'])['value'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drought length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count consectuive values\n",
    "def count_dur(y):\n",
    "    x = y * (y.groupby((y != y.shift()).cumsum()).cumcount() + 1)\n",
    "    return x\n",
    "\n",
    "#add drought column\n",
    "df['drought'] = 0\n",
    "df['drought'][df['value'] <= -1] = 1\n",
    "\n",
    "#cycle through each model and region and calculate length of droughts (consecutive values < -1)\n",
    "#split df by model and region (will have to had experiment as well if add ssp8.5 etc)\n",
    "mod = np.unique(df['model'])\n",
    "reg = np.unique(df['region'])\n",
    "\n",
    "drought_df = pd.DataFrame(columns = ['model', 'region', 'mean_dur', 'max_dur', 'num_5_or_more'])\n",
    "for m in mod:\n",
    "    df_mod = df[df['model'] == m]\n",
    "    for r in reg:\n",
    "        df_r = df_mod[df_mod['region'] == r]\n",
    "        df_r['duration'] = count_dur(df_r['drought'])\n",
    "        \n",
    "        #calculate characteristics of model\n",
    "        x_max = np.nanmax(df_r['duration'])\n",
    "        mean_dur = np.nanmean(df_r[df_r['duration'] > 0]['duration'])\n",
    "        df_5 = df_r[df_r['duration'] >= 5]\n",
    "\n",
    "        new_df = pd.DataFrame({'model': [m],\n",
    "                              'region': r,\n",
    "                              'mean_dur': mean_dur,\n",
    "                              'max_dur': x_max,\n",
    "                              'num_5_or_more': len(df_5.index)})\n",
    "\n",
    "        drought_df = drought_df.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean drought length: 1.0 months\n",
      "Mean maximum drought length: 1.0 months\n",
      "For those models who have droughts lasting 5 months or more, droughts of this length occur on average  5.0 times during the 30 year historical period.\n"
     ]
    }
   ],
   "source": [
    "x5 = np.nanmean(drought_df[drought_df['num_5_or_more'] > 0 ]['num_5_or_more'].astype('float'))\n",
    "\n",
    "\n",
    "print('Mean drought length:', np.round(np.nanmean(drought_df['mean_dur'], 0)), 'months')\n",
    "print('Mean maximum drought length:', np.round(np.nanmean(drought_df['max_dur'].astype('float'),0)), 'months')\n",
    "print('For those models who have droughts lasting 5 months or more, droughts of this length occur on average ', np.round(x5,0), 'times during the 30 year historical period.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
